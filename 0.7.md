---
layout: default
title: 0.7
core_version: 0.7.9
agent_version: 2.2.0
ui_version: 0.7.5
arango_version: "3.11"
migrator_version: 0.4.2
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/Bz_Ml6pNH2E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**Spline** is a free and open-source tool for automated tracking data lineage and data pipeline structure in your organization.

Originally the project was created as a lineage tracking tool specifically for Apache Spark &trade; (the name Spline stands for Spark Lineage).
In 2018, the [IEEE Paper](https://ieeexplore.ieee.org/abstract/document/8367160) has been published.
Later though, the vision of the project was expanded, and the system design
was generalized to accommodate other data technologies, not only Spark.

The goal of the project is to create a simple but capable cross-platform and cross-framework
data-lineage tracking solution that could be used along, or serve as a foundation for building more advanced data governance solutions on top of it.

At a high-level, the project consists of three main parts:

-   [Spline Server](https://github.com/AbsaOSS/spline/tree/release/{{page.core_version}})
-   Spline Agents:
    - [Apache Spark agent](https://github.com/AbsaOSS/spline-spark-agent/tree/release/{{page.agent_version}}) &ndash; for Scala, Python or Spark SQL dialect
    - [Python agent](https://github.com/AbsaOSS/spline-python-agent) &ndash; for arbitrary functions written in Python 3.9+
    - [Open-Lineage adapter (PoC)](https://github.com/AbsaOSS/spline-openlineage) &ndash; for importing metadata in OpenLineage API format.
-   [Spline UI](https://github.com/AbsaOSS/spline-ui/tree/release/{{page.ui_version}})

The Spline Server is the heart of Spline. It receives the lineage data from agents via _Producer API_ and stores it in the ArangoDB.
On the other end It provides _Consumer API_ for reading and querying the lineage data.
_Consumer API_ is used by Spline UI, but can also be used by 3rd party applications.

The agents capture the lineage meta-data form the data-transformation pipelines and send it to the Spline server in a standardized format
via a cross-platform API (called Producer API) using HTTP (REST) or Kafka as transport.

![Spline diagram](https://user-images.githubusercontent.com/5530211/70050339-fd93f580-15ce-11ea-88b2-4d79ee30d494.png)

The lineage data is then processed and stored in a graph format, and is accessible via another REST API (called Consumer API).

<a id="toc"></a>
# Table of Contents

<!--ts-->
* [Motivation](#motivation)
* [Getting started](#getting-started)
   * [TL;DR](#tldr)
   * [Step-by-step instructions](#step-by-step)
      * [Get Spline components](#get-spline-components)
      * [Create Spline Database](#create-spline-database)
      * [Start Spline Server](#start-spline-server)
         * [Configuration](#configuration)
         * [Check Spline Server status](#check-spline-server-status)
      * [Start Spline UI](#start-spline-ui)
      * [Capture some lineage](#capture-lineage)
   * [Running Spline as Java application](#java)
* [Upgrade Spline database](#upgrade)
   * [Upgrade from any Spline version greater than  0.4 to the latest version](#upgrade-from-0.4)
   * [Upgrade from Spline 0.3](#upgrade-from-0.3)

<!-- Added by: wajda, at: Wed 19 May 16:52:17 CEST 2021 -->

<!--te-->

<a id="motivation"></a>
# Motivation

Our main focus is to solve the following particular problems:

-   **Regulatory requirements** (BCBS 239, GDPR etc)

-   **Documentation of business logic**

    Business analysts should get a chance to verify whether jobs were written according to the rules they provided.
    Moreover, it would be beneficial for them to have up-to-date documentation where they can refresh their knowledge of a project.

-   **Identification of performance bottlenecks**

    Our focus is not only business-oriented; we also see Spline as a development tool that should be able to help developers with the performance optimization of their Spark jobs.

---

<a id="getting-started"></a>
# Getting started

<a id="tldr"></a>
## TL;DR

We have created a _Docker-compose_ config (see [Spline getting started](https://github.com/AbsaOSS/spline-getting-started))
to help you to get Spline up and running in just a few keystrokes.
So if you only have 5 minutes to try Spline out then simply do the following:
```shell
wget https://raw.githubusercontent.com/AbsaOSS/spline-getting-started/main/docker/docker-compose.yml

wget https://raw.githubusercontent.com/AbsaOSS/spline-getting-started/main/docker/.env

docker-compose up
```

That will spin up a few Docker containers (ArangoDB, Spline Server and Spline UI), and run a set of Spark examples to pre-populate the database.

Open [http://localhost:9090](http://localhost:9090) in the browser to see the Spline UI and captured example lineage data.

The Spline server is running on [http://localhost:8080](http://localhost:8080)

Now you can run your tracked job implemented in Python, or Apache Spark (`spark-shell`, `pyspark` or `spark-submit`) with lineage tracking enabled
and immediately see the result in the Spline UI.

##### Python example
First, decorate you Python code that you want to track with the Spline Agent decorators.
At minimum, you need to specify the entry point function (a top-level function that will be tracked) and 
the input and output datasource URIs (for the purpose of tracking lineage).  

*my-example-app.py:*
```python
import spline_agent
from spline_agent.enums import WriteMode

@spline_agent.track_lineage(name = "My Awesome Python Job")
def my_example_func():
    # read some files, do stuff, write the result
    _read_from("hdfs://..../input.csv")
    _write_to("s3://..../output.csv")


@spline_agent.inputs('{url}')
def _read_from(url: str) -> SomeData:
    # do actual read
    return data


@spline_agent.output('{url}', WriteMode.OVERWRITE)
def _write_to(url: str, payload: SomeData):
    # do actual write
    pass
```
then, install Spline Agent library, configure it, and execute your Python app as usual. 
```shell
# install "spline-agent" library
pip install spline-agent
# configure the Spline Agent to send captured lineage metadata to the Spline Producer REST API 
export SPLINE_LINEAGE_DISPATCHER_TYPE=http
export SPLINE_LINEAGE_DISPATCHER_HTTP_BASE_URL=http://localhost:8080/producer
# execute the Python app. The lineage will captured and sent to the Spline server.
python my-example-app.py
```

##### Spark example
```shell
# Start the "pyspark" shell (or we can use "spark-shell" or "spark-submit" in the same way)
# with lineage tracking enabled automatically for any execute Spark job.
pyspark \
  --packages za.co.absa.spline.agent.spark:spark-3.0-spline-agent-bundle_2.12:{{page.agent_version}} \
  --conf "spark.sql.queryExecutionListeners=za.co.absa.spline.harvester.listener.SplineQueryExecutionListener" \
  --conf "spark.spline.producer.url=http://localhost:8080/producer"
```
**Note**: By default, only persistent writes are tracked (like write to a file, Hive table or a database).

**Note**: In the example above we used Spline Agent bundle compiled for Spark 3.0 and Scala 2.12.
For other Spark and Scala versions use corresponding bundles.
See [selecting agent artifacts](https://github.com/AbsaOSS/spline-spark-agent#selecting-artifact) for details.

Also see [Spline getting started](https://github.com/AbsaOSS/spline-getting-started)

<a id="step-by-step"></a>
## Step-by-step instructions

<a id="get-spline-components"></a>
### Get Spline components

All Spline components are available as J2EE artifacts (JAR or WAR-files) as well as Docker containers:

Choose what suites you better.

Docker containers are hosted on Docker Hub:
-  [absaoss/spline-admin:{{page.core_version}}](https://hub.docker.com/r/absaoss/spline-admin)
-  [absaoss/spline-rest-server:{{page.core_version}}](https://hub.docker.com/r/absaoss/spline-rest-server)
-  [absaoss/spline-kafka-server:{{page.core_version}}](https://hub.docker.com/r/absaoss/spline-kafka-server)
-  [absaoss/spline-web-ui:{{page.ui_version}}](https://hub.docker.com/r/absaoss/spline-web-ui)
-  [absaoss/spline-spark-agent:{{page.agent_version}}](https://hub.docker.com/r/absaoss/spline-spark-agent) (contains examples)

J2EE artifacts can be found on Maven Central:
-  [za.co.absa.spline:admin:{{page.core_version}}](https://repo1.maven.org/maven2/za/co/absa/spline/admin/{{page.core_version}}/)
-  [za.co.absa.spline:rest-gateway:{{page.core_version}}](https://repo1.maven.org/maven2/za/co/absa/spline/rest-gateway/{{page.core_version}}/)
-  [za.co.absa.spline:kafka-gateway:{{page.core_version}}](https://repo1.maven.org/maven2/za/co/absa/spline/kafka-gateway/{{page.core_version}}/)
-  [za.co.absa.spline.ui:spline-web-ui:{{page.ui_version}}](https://repo1.maven.org/maven2/za/co/absa/spline/ui/spline-web-ui/{{page.ui_version}}/)

Python agent is on available on PyPI:
-  [spline-python-agent](https://pypi.org/project/spline-agent/)

<a id="create-spline-database"></a>
### Create Spline Database
Spline server requires *ArangoDB* to run.

Please install _ArangoDB version {{page.arango_version}}_ or newer according to the instructions in [ArangoDB documentation](https://www.arangodb.com/docs/stable/getting-started-installation.html).

Example:
```shell
docker run -p 8529:8529 -e ARANGO_NO_AUTH=1 arangodb:{{page.arango_version}}
```

Once the database server is running you should be able to see ArangoDB Web UI at [http://localhost:8529](http://localhost:8529)

Next, create a Spline database using Spline Admin utility:
```shell
docker run -it --rm absaoss/spline-admin:{{page.core_version}} db-init arangodb://172.17.0.1/spline
```

<a id="admin-tool-help"></a>
Detailed usage documentation can be seen by running the Admin tool with the `--help` parameter:
```shell
docker run -it --rm absaoss/spline-admin:{{page.core_version}} --help
```

<a id="start-spline-server"></a>
### Start Spline Server
```shell
docker container run \
  -e SPLINE_DATABASE_CONNECTION_URL=arangodb://host.docker.internal/spline \
  -p 8080:8080 \
  absaoss/spline-rest-server:{{page.core_version}}
```
Optionally, you may also start Spline Kafka Gateway if you plan to use Kafka transport for your agents.

See [Spline Kafka](https://github.com/AbsaOSS/spline/tree/release/{{page.core_version}}/kafka-gateway) for details.
```shell
docker container run \
  -e SPLINE_DATABASE_CONNECTION_URL=arangodb://host.docker.internal/spline \
  -e spline.kafka.consumer.bootstrap.servers=localhost:9092 \
  -e spline.kafka.consumer.group.id=spline-group \
  -e spline.kafka.topic=spline-topic \
  -p 7070:8080 \
  absaoss/spline-kafka-server:{{page.core_version}}
```

**Note** for Linux users: If `host.docker.internal` does not resolve replace it with `172.17.0.1` (see [Docker for-linux bug report](https://github.com/docker/for-linux/issues/264))

<a id="configuration"></a>
#### Configuration

-   `SPLINE_DATABASE_CONNECTION_URL`
    
    URL to the ArangoDB database. (See [Admin tool help](#admin-tool-help))

-   `SPLINE_DATABASE_LOG_FULL_QUERY_ON_ERROR`

    Useful for development and debugging purposes. It controls the AQL log verbosity in case of errors. 
    Set it to `true` if you want the full AQL query to be logged. (Default is `false` - only a part of the query is logged).

-   `SPLINE_DATABASE_DISABLE_SSL_VALIDATION`

    Should validation of self-signed SSL certificates be disabled.
    `true` - SSL validation is disabled (Don't use on production). `false` - enabled (Default).

-   `spline.kafka.topic`

    A topic from which the gateway should consume messages.

-   `spline.kafka.consumer.*`

    A prefix for standard kafka consumer properties. Kafka gateway uses standard Kafka consumer inside,
    so any of it's [confiuration properties](https://kafka.apache.org/documentation/#consumerconfigs) can be used after this prefix.
    
    For example `spline.kafka.consumer.bootstrap.servers` or `spline.kafka.consumer.group.id`.

<a id="check-spline-server-status"></a>
#### Check Spline Server status

Open the server URL in the browser: http://localhost:8080

You should see a dashboard with the updating server status information, server version, exposed API and some other useful info.

The Spline REST server exposes the following REST APIs:
- Producer API (`/producer/*`)
- Consumer API (`/consumer/*`)

... and other useful URLs:
- Running server version information: [/about/version](http://localhost:8080/about/version)
- Producer API Swagger documentation: [/docs/producer.html](http://localhost:8080/docs/producer.html)
- Consumer API Swagger documentation: [/docs/consumer.html](http://localhost:8080/docs/consumer.html)

<a id="start-spline-ui"></a>
### Start Spline UI

```shell
docker container run \
      -e SPLINE_CONSUMER_URL=http://localhost:8080/consumer \
      -p 9090:8080 \
      absaoss/spline-web-ui:{{page.ui_version}}
```

Open Spline Web UI in the browser: http://localhost:9090

See [Spline UI Readme](https://github.com/AbsaOSS/spline-ui/tree/release/{{page.ui_version}}/) for details.

<a id="capture-lineage"></a>
### Capture some lineage

Refer to the [Spline agent for Spark](https://github.com/AbsaOSS/spline-spark-agent/tree/release/{{page.agent_version}}) 
for detailed explanation how Spline agent is used to capture lineage from Spark jobs.

Also see [Examples page](https://github.com/AbsaOSS/spline-spark-agent/tree/release/{{page.agent_version}}/examples),
where you can find examples of generic (non-Spark) lineage capturing, by directly calling Spline Producer API.  

<a id="java"></a>
## Running Spline as Java application

Although Docker is the preferred and the most convenient way of running Spline, you can also run it on standard Java environment.

**The admin tool**:
```shell
java -jar admin-{{page.core_version}}.jar db-init arangodb://localhost/spline
```

**The REST server**:

Download the WAR-file using the link below, and deploy it onto any J2EE-compatible Web Container,
e.g. Tomcat, Jetty, JBoss etc.

[za.co.absa.spline:rest-gateway:{{page.core_version}}](https://repo1.maven.org/maven2/za/co/absa/spline/rest-gateway/{{page.core_version}}/rest-gateway-{{page.core_version}}.war)

Spline Web application is looking for configuration in the following sources (in order of precedence):
- Naming and directory context (via JNDI) if available

  (for example in a `context.xml` in the Tomcat server)
    ```xml
    <Environment type="java.lang.String"
                 name="spline/database/connectionUrl"
                 value="arangodb://localhost/spline" />
    ```

- JVM system properties
    ```shell
    $JAVA_OPTS -Dspline.database.connectionUrl=arangodb://localhost/spline
    ```

- System environment variables
    ```shell
    export SPLINE_DATABASE_CONNECTION_URL=arangodb://localhost/spline
    ```

Please adhere to the naming convention of each configuration source type. For example, the property `foo.barBaz` would be looked up as `foo.barBaz` in the JVM options, as `foo/barBaz` in the JNDI, and as `FOO_BAR_BAZ` in the environment variables.

**The Web UI**:

See [Spline UI - Running a WAR-file](https://github.com/AbsaOSS/spline-ui/tree/release/{{page.ui_version}}#run-as-war)

<a id="upgrade"></a>
# Upgrade Spline database

<a id="upgrade-from-0.4"></a>
## Upgrade from any Spline version greater than  0.4 to the latest version

```shell
docker run -it --rm absaoss/spline-admin:{{page.core_version}} db-upgrade <SPLINE_DB_URL>
```
... or if you prefer Java
```shell 
java -jar admin-{{page.core_version}}.jar db-upgrade <SPLINE_DB_URL>
```

**Note**: Depending on your database size, migration procedure (especially from 0.5 to 0.6) can consume significant amount of memory on the ArangoDB server.
It's recommended to preliminary increase RAM size (e.g. at least 128Gb for a database with average 500K records per collection). When migration is complete, RAM can be returned to its normal size.

**Note**: There is no automatic rollback for database migration! If migration fails for any reason you should start over from the clean database snapshot. So don't forget to make backups before running the migration tool!

<a id="upgrade-from-0.3"></a>
## Upgrade from Spline 0.3

Spline 0.3 stored data in the MongoDB. Starting from version 0.4 further Spline uses ArangoDB instead.
You need to use another tool for migrating data between those two databases - [Spline Migrator](https://repo1.maven.org/maven2/za/co/absa/spline/migrator-tool/0.4.2/migrator-tool-0.4.2.jar)

You also need to have a new Spline server running. The migrator tool will fetch the data directly from a MongoDB,
convert it and send to a new Spline Producer REST endpoint.

```shell 
java -jar migrator-tool-{{page.migrator_version}}.jar \
  --source=mongodb://localhost:27017/splinedb \
  --target=http://localhost:8080/producer
```

Also run `java -jar migrator-tool-{{page.migrator_version}}.jar --help` to read about usage and available options.

For more information you may take a look at the [Migrator tool source code](https://github.com/AbsaOSS/spline/tree/release/{{page.migrator_version}}/migrator-tool).

---

    Copyright 2019 ABSA Group Limited
    
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
